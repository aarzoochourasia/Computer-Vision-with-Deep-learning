{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Copy of Transfer Learning with VGG16.ipynb","provenance":[{"file_id":"0B_yc69xra5jOSDQ3eWtQaXFzWEJTVkNfd2RkYUVPZDF4dDJv","timestamp":1580112874971}]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"EC6LGJdmmYo5","colab_type":"text"},"source":["# Making a Flower Classifier with VGG16\n","\n","### Loading the VGG16 Model"]},{"cell_type":"code","metadata":{"id":"HC8uLNjwmYpC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":411},"outputId":"acafd2d0-1147-49a1-b512-54a8317a0ae7","executionInfo":{"status":"ok","timestamp":1580111607026,"user_tz":-330,"elapsed":10794,"user":{"displayName":"Aarzoo Chourasia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDedYTC4eNHvhcUObzzXt-VIGgy9yV8wysLolGOdQ=s64","userId":"16659348880850199723"}}},"source":["from keras.applications import VGG16\n","\n","# VGG16 was designed to work on 224 x 224 pixel input images sizes\n","img_rows = 224\n","img_cols = 224 \n","\n","#Loads the VGG16 model \n","vgg16 = VGG16(weights = 'imagenet', \n","                 include_top = False, \n","                 input_shape = (img_rows, img_cols, 3))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gny7mH7mmYpQ","colab_type":"text"},"source":["### Inpsecting each layer"]},{"cell_type":"code","metadata":{"id":"kuG7vXWLmYpS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"9ed18c36-1441-480e-f529-9ce14f6c80a2","executionInfo":{"status":"ok","timestamp":1580111612999,"user_tz":-330,"elapsed":1010,"user":{"displayName":"Aarzoo Chourasia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDedYTC4eNHvhcUObzzXt-VIGgy9yV8wysLolGOdQ=s64","userId":"16659348880850199723"}}},"source":["# Let's print our layers \n","for (i,layer) in enumerate(vgg16.layers):\n","    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["0 InputLayer False\n","1 Conv2D True\n","2 Conv2D True\n","3 MaxPooling2D True\n","4 Conv2D True\n","5 Conv2D True\n","6 MaxPooling2D True\n","7 Conv2D True\n","8 Conv2D True\n","9 Conv2D True\n","10 MaxPooling2D True\n","11 Conv2D True\n","12 Conv2D True\n","13 Conv2D True\n","14 MaxPooling2D True\n","15 Conv2D True\n","16 Conv2D True\n","17 Conv2D True\n","18 MaxPooling2D True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8XPDrEFimYpy","colab_type":"text"},"source":["### Let's freeze all layers except the top 4 "]},{"cell_type":"code","metadata":{"id":"6I-L6jVBmYp5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"ebf08b61-b87c-4d33-a659-845f140a7930","executionInfo":{"status":"ok","timestamp":1580111618130,"user_tz":-330,"elapsed":1473,"user":{"displayName":"Aarzoo Chourasia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDedYTC4eNHvhcUObzzXt-VIGgy9yV8wysLolGOdQ=s64","userId":"16659348880850199723"}}},"source":["from keras.applications import VGG16\n","\n","# VGG16 was designed to work on 224 x 224 pixel input images sizes\n","img_rows = 224\n","img_cols = 224 \n","\n","# Re-loads the VGG16 model without the top or FC layers\n","vgg16 = VGG16(weights = 'imagenet', \n","                 include_top = False, \n","                 input_shape = (img_rows, img_cols, 3))\n","\n","# Here we freeze the last 4 layers \n","# Layers are set to trainable as True by default\n","for layer in vgg16.layers:\n","    layer.trainable = False\n","    \n","# Let's print our layers \n","for (i,layer) in enumerate(vgg16.layers):\n","    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["0 InputLayer False\n","1 Conv2D False\n","2 Conv2D False\n","3 MaxPooling2D False\n","4 Conv2D False\n","5 Conv2D False\n","6 MaxPooling2D False\n","7 Conv2D False\n","8 Conv2D False\n","9 Conv2D False\n","10 MaxPooling2D False\n","11 Conv2D False\n","12 Conv2D False\n","13 Conv2D False\n","14 MaxPooling2D False\n","15 Conv2D False\n","16 Conv2D False\n","17 Conv2D False\n","18 MaxPooling2D False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lHJnGXPrmYqE","colab_type":"text"},"source":["### Let's make a function that returns our FC Head"]},{"cell_type":"code","metadata":{"id":"ccr2L2LFmYqF","colab_type":"code","colab":{}},"source":["def addTopModel(bottom_model, num_classes, D=256):\n","    \"\"\"creates the top or head of the model that will be \n","    placed ontop of the bottom layers\"\"\"\n","    top_model = bottom_model.output\n","    top_model = Flatten(name = \"flatten\")(top_model)\n","    top_model = Dense(D, activation = \"relu\")(top_model)\n","    top_model = Dropout(0.3)(top_model)\n","    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n","    return top_model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"INvt2MzTmYqN","colab_type":"text"},"source":["### Let's add our FC Head back onto VGG"]},{"cell_type":"code","metadata":{"id":"-ZWCLLj9mYrH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"5c2411ab-dc86-488f-99c5-8655bb3a7aae","executionInfo":{"status":"ok","timestamp":1580111625359,"user_tz":-330,"elapsed":1226,"user":{"displayName":"Aarzoo Chourasia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDedYTC4eNHvhcUObzzXt-VIGgy9yV8wysLolGOdQ=s64","userId":"16659348880850199723"}}},"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n","from keras.layers.normalization import BatchNormalization\n","from keras.models import Model\n","\n","num_classes = 17\n","\n","FC_Head = addTopModel(vgg16, num_classes)\n","\n","model = Model(inputs=vgg16.input, outputs=FC_Head)\n","\n","print(model.summary())"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 224, 224, 3)       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 256)               6422784   \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 17)                4369      \n","=================================================================\n","Total params: 21,141,841\n","Trainable params: 6,427,153\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jbw21KWw5I87","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"27726beb-6e0a-4feb-d328-819c9768091a","executionInfo":{"status":"ok","timestamp":1580111659054,"user_tz":-330,"elapsed":29214,"user":{"displayName":"Aarzoo Chourasia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDedYTC4eNHvhcUObzzXt-VIGgy9yV8wysLolGOdQ=s64","userId":"16659348880850199723"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0avxQrVhmYrj","colab_type":"text"},"source":["### Loading our Flowers Dataset"]},{"cell_type":"code","metadata":{"id":"hr5iu5-ImYsK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"32dc1ad5-fcf4-4e4b-8d3b-9e8116dd78c7","executionInfo":{"status":"ok","timestamp":1580111667534,"user_tz":-330,"elapsed":5389,"user":{"displayName":"Aarzoo Chourasia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDedYTC4eNHvhcUObzzXt-VIGgy9yV8wysLolGOdQ=s64","userId":"16659348880850199723"}}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","train_data_dir = 'drive/My Drive/17_flowers/train'\n","validation_data_dir = 'drive/My Drive/17_flowers/validation'\n","\n","train_datagen = ImageDataGenerator(\n","      rescale=1./255,\n","      rotation_range=20,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      horizontal_flip=True,\n","      fill_mode='nearest')\n"," \n","validation_datagen = ImageDataGenerator(rescale=1./255)\n"," \n","# Change the batchsize according to your system RAM\n","train_batchsize = 16\n","val_batchsize = 10\n"," \n","train_generator = train_datagen.flow_from_directory(\n","        train_data_dir,\n","        target_size=(img_rows, img_cols),\n","        batch_size=train_batchsize,\n","        class_mode='categorical')\n"," \n","validation_generator = validation_datagen.flow_from_directory(\n","        validation_data_dir,\n","        target_size=(img_rows, img_cols),\n","        batch_size=val_batchsize,\n","        class_mode='categorical',\n","        shuffle=False)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Found 1190 images belonging to 17 classes.\n","Found 180 images belonging to 17 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uTsIWnTSmYsk","colab_type":"text"},"source":["### Training our top layers"]},{"cell_type":"code","metadata":{"id":"gXSxhB7VmYsl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":428},"outputId":"dea0bcd5-beb8-4cce-f49b-eb3bd9da289c","executionInfo":{"status":"ok","timestamp":1580111960805,"user_tz":-330,"elapsed":284921,"user":{"displayName":"Aarzoo Chourasia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDedYTC4eNHvhcUObzzXt-VIGgy9yV8wysLolGOdQ=s64","userId":"16659348880850199723"}}},"source":["from keras.optimizers import RMSprop\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","                   \n","checkpoint = ModelCheckpoint(\"drive/My Drive/flowers_vgg_64.h5\",\n","                             monitor=\"val_loss\",\n","                             mode=\"min\",\n","                             save_best_only = True,\n","                             verbose=1)\n","\n","earlystop = EarlyStopping(monitor = 'val_loss', \n","                          min_delta = 0, \n","                          patience = 3,\n","                          verbose = 1,\n","                          restore_best_weights = True)\n","\n","# we put our call backs into a callback list\n","callbacks = [earlystop, checkpoint]\n","\n","# Note we use a very small learning rate \n","model.compile(loss = 'categorical_crossentropy',\n","              optimizer = RMSprop(lr = 0.001),\n","              metrics = ['accuracy'])\n","\n","nb_train_samples = 1190\n","nb_validation_samples = 170\n","epochs = 3\n","batch_size = 16\n","\n","history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch = nb_train_samples // batch_size,\n","    epochs = epochs,\n","    callbacks = callbacks,\n","    validation_data = validation_generator,\n","    validation_steps = nb_validation_samples // batch_size)\n","\n","model.save(\"drive/My Drive/flowers_vgg_64.h5\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Epoch 1/3\n","74/74 [==============================] - 234s 3s/step - loss: 9.8618 - acc: 0.1278 - val_loss: 1.8529 - val_acc: 0.4100\n","\n","Epoch 00001: val_loss improved from inf to 1.85293, saving model to drive/My Drive/flowers_vgg_64.h5\n","Epoch 2/3\n","74/74 [==============================] - 23s 307ms/step - loss: 2.0282 - acc: 0.3731 - val_loss: 1.6722 - val_acc: 0.5500\n","\n","Epoch 00002: val_loss improved from 1.85293 to 1.67222, saving model to drive/My Drive/flowers_vgg_64.h5\n","Epoch 3/3\n","74/74 [==============================] - 20s 276ms/step - loss: 1.6380 - acc: 0.4997 - val_loss: 1.1477 - val_acc: 0.6500\n","\n","Epoch 00003: val_loss improved from 1.67222 to 1.14771, saving model to drive/My Drive/flowers_vgg_64.h5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rqXsIIjnmYsq","colab_type":"text"},"source":["## Can we speed this up?\n","#### Let's try re-sizing the image to 64 x 64"]},{"cell_type":"code","metadata":{"id":"WUOGGkIjmYs2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"cdf2756b-11c3-4703-beb7-93443aeecbc6","executionInfo":{"status":"ok","timestamp":1580112141243,"user_tz":-330,"elapsed":1343,"user":{"displayName":"Aarzoo Chourasia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDedYTC4eNHvhcUObzzXt-VIGgy9yV8wysLolGOdQ=s64","userId":"16659348880850199723"}}},"source":["from keras.applications import VGG16\n","\n","# Setting the input size now to 64 x 64 pixel \n","img_rows = 64\n","img_cols = 64 \n","\n","# Re-loads the VGG16 model without the top or FC layers\n","vgg16 = VGG16(weights = 'imagenet', \n","                 include_top = False, \n","                 input_shape = (img_rows, img_cols, 3))\n","\n","# Here we freeze the last 4 layers \n","# Layers are set to trainable as True by default\n","for layer in vgg16.layers:\n","    layer.trainable = False\n","    \n","# Let's print our layers \n","for (i,layer) in enumerate(vgg16.layers):\n","    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["0 InputLayer False\n","1 Conv2D False\n","2 Conv2D False\n","3 MaxPooling2D False\n","4 Conv2D False\n","5 Conv2D False\n","6 MaxPooling2D False\n","7 Conv2D False\n","8 Conv2D False\n","9 Conv2D False\n","10 MaxPooling2D False\n","11 Conv2D False\n","12 Conv2D False\n","13 Conv2D False\n","14 MaxPooling2D False\n","15 Conv2D False\n","16 Conv2D False\n","17 Conv2D False\n","18 MaxPooling2D False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IfLw2z5gmYs8","colab_type":"text"},"source":["### Let's create our new model using an image size of 64 x 64"]},{"cell_type":"code","metadata":{"id":"9ywSn8-WmYs9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":986},"outputId":"d2aeda12-f77f-4f29-d463-e0b46bbed12d","executionInfo":{"status":"ok","timestamp":1580112147959,"user_tz":-330,"elapsed":1712,"user":{"displayName":"Aarzoo Chourasia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDedYTC4eNHvhcUObzzXt-VIGgy9yV8wysLolGOdQ=s64","userId":"16659348880850199723"}}},"source":["from keras.applications import VGG16\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n","from keras.layers.normalization import BatchNormalization\n","from keras.models import Model\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","train_data_dir = 'drive/My Drive/17_flowers/train'\n","validation_data_dir = 'drive/My Drive/17_flowers/validation'\n","\n","train_datagen = ImageDataGenerator(\n","      rescale=1./255,\n","      rotation_range=20,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      horizontal_flip=True,\n","      fill_mode='nearest')\n"," \n","validation_datagen = ImageDataGenerator(rescale=1./255)\n"," \n","# Change the batchsize according to your system RAM\n","train_batchsize = 16\n","val_batchsize = 10\n"," \n","train_generator = train_datagen.flow_from_directory(\n","        train_data_dir,\n","        target_size=(img_rows, img_cols),\n","        batch_size=train_batchsize,\n","        class_mode='categorical')\n"," \n","validation_generator = validation_datagen.flow_from_directory(\n","        validation_data_dir,\n","        target_size=(img_rows, img_cols),\n","        batch_size=val_batchsize,\n","        class_mode='categorical',\n","        shuffle=False)\n","\n","# Re-loads the VGG16 model without the top or FC layers\n","vgg16 = VGG16(weights = 'imagenet', \n","                 include_top = False, \n","                 input_shape = (img_rows, img_cols, 3))\n","\n","# Freeze layers\n","for layer in vgg16.layers:\n","    layer.trainable = False\n","    \n","# Number of classes in the Flowers-17 dataset\n","num_classes = 17\n","\n","FC_Head = addTopModel(vgg16, num_classes)\n","\n","model = Model(inputs=vgg16.input, outputs=FC_Head)\n","\n","print(model.summary())"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Found 1190 images belonging to 17 classes.\n","Found 180 images belonging to 17 classes.\n","Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         (None, 64, 64, 3)         0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 2048)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 256)               524544    \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 17)                4369      \n","=================================================================\n","Total params: 15,243,601\n","Trainable params: 528,913\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L8e7iaY5mYtM","colab_type":"text"},"source":["### Training using 64 x 64 image size is MUCH faster!"]},{"cell_type":"code","metadata":{"id":"H8xDMlCLmYtN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":561},"outputId":"722150a6-4540-4e86-b7a9-6ed6dee69c8d","executionInfo":{"status":"ok","timestamp":1580112680145,"user_tz":-330,"elapsed":34905,"user":{"displayName":"Aarzoo Chourasia","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDedYTC4eNHvhcUObzzXt-VIGgy9yV8wysLolGOdQ=s64","userId":"16659348880850199723"}}},"source":["from keras.optimizers import RMSprop\n","from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n","                   \n","checkpoint = ModelCheckpoint(\"drive/My Drive/flowers_vgg_64.h5\",\n","                             monitor=\"val_loss\",\n","                             mode=\"min\",\n","                             save_best_only = True,\n","                             verbose=1)\n","\n","earlystop = EarlyStopping(monitor = 'val_loss', \n","                          min_delta = 0, \n","                          patience = 5,\n","                          verbose = 1,\n","                          restore_best_weights = True)\n","\n","reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n","                              factor = 0.2,\n","                              patience = 3,\n","                              verbose = 1,\n","                              min_delta = 0.00001)\n","\n","# we put our call backs into a callback list\n","callbacks = [earlystop, checkpoint, reduce_lr]\n","\n","# Note we use a very small learning rate \n","model.compile(loss = 'categorical_crossentropy',\n","              optimizer = RMSprop(lr = 0.0001),\n","              metrics = ['accuracy'])\n","\n","nb_train_samples = 1190\n","nb_validation_samples = 170\n","epochs = 25\n","batch_size = 32\n","\n","history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch = nb_train_samples // batch_size,\n","    epochs = epochs,\n","    callbacks = callbacks,\n","    validation_data = validation_generator,\n","    validation_steps = nb_validation_samples // batch_size)\n","\n","model.save(\"drive/My Drive/flowers_vgg_64.h5\")"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Epoch 1/25\n","37/37 [==============================] - 5s 140ms/step - loss: 1.6055 - acc: 0.5080 - val_loss: 1.6651 - val_acc: 0.5000\n","\n","Epoch 00001: val_loss improved from inf to 1.66512, saving model to drive/My Drive/flowers_vgg_64.h5\n","Epoch 2/25\n","37/37 [==============================] - 4s 109ms/step - loss: 1.5451 - acc: 0.5169 - val_loss: 0.9572 - val_acc: 0.7800\n","\n","Epoch 00002: val_loss improved from 1.66512 to 0.95716, saving model to drive/My Drive/flowers_vgg_64.h5\n","Epoch 3/25\n","37/37 [==============================] - 4s 113ms/step - loss: 1.4733 - acc: 0.5642 - val_loss: 1.1698 - val_acc: 0.7400\n","\n","Epoch 00003: val_loss did not improve from 0.95716\n","Epoch 4/25\n","37/37 [==============================] - 4s 116ms/step - loss: 1.5192 - acc: 0.5304 - val_loss: 1.3275 - val_acc: 0.5600\n","\n","Epoch 00004: val_loss did not improve from 0.95716\n","Epoch 5/25\n","37/37 [==============================] - 4s 114ms/step - loss: 1.4801 - acc: 0.5541 - val_loss: 1.3882 - val_acc: 0.5600\n","\n","Epoch 00005: val_loss did not improve from 0.95716\n","\n","Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n","Epoch 6/25\n","37/37 [==============================] - 4s 108ms/step - loss: 1.3877 - acc: 0.6149 - val_loss: 1.0468 - val_acc: 0.7800\n","\n","Epoch 00006: val_loss did not improve from 0.95716\n","Epoch 7/25\n","37/37 [==============================] - 4s 109ms/step - loss: 1.3599 - acc: 0.5900 - val_loss: 1.3914 - val_acc: 0.6200\n","Restoring model weights from the end of the best epoch\n","\n","Epoch 00007: val_loss did not improve from 0.95716\n","Epoch 00007: early stopping\n"],"name":"stdout"}]}]}